---
title: "Colorful Melodies: Exploring the Colors of Music"
author: "Yuyue Xiao"

output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      version: 4
      bg: "#101010"
      fg: "#FDF7F7" 
      primary: "#ED79F9"
      navbar-bg: "#3ADAC6"
      base_font: 
        google: Prompt
      heading_font:
        google: Sen
      code_font:
        google: 
          # arguments to sass::font_google() 
          family: JetBrains Mono
          local: false
---

```{r}
library(tidyverse)
library(spotifyr)
library(compmus)
library(recipes)
library(rsample)
library(heatmaply)
library(tidymodels)
library(ggdendro)
library(parsnip)
library(workflows)
library(yardstick)
library(rsample)
library(workflows)
library(kknn)
library(yardstick)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

```

### Classification

```{r}
black <- get_playlist_audio_features("spotify", "36pH6ey9uoMBnZDj4pAXyv")
blue <- get_playlist_audio_features("spotify", "0wtw9NQFVy2zMQxZ8BAe5T")
pink <- get_playlist_audio_features("spotify", "2OZ42DxmlNZUua113zuz1A")
white <- get_playlist_audio_features("spotify", "0SqwX4Y54AjbnXsha6QU4I")
music <-
  bind_rows(
    black |> mutate(playlist = "Black"),
    blue |> mutate(playlist = "Blue") ,
    pink |> mutate(playlist = "Pink"),
    white |> mutate(playlist = "White")
  ) |> 
  add_audio_analysis()
```

```{r}
music_features <-
  music |>  
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```



```{r}
music_recipe <- 
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      mode + 
      key +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = music_features
  ) %>%
  step_center(all_predictors()) %>%# Converts to z-scores.
  step_scale(all_predictors())# Sets range to [0, 1].
```

```{r}
music_cv <- music_features |> vfold_cv(5)
```


```{r}
knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
music_knn <- 
  workflow() |> 
  add_recipe(music_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(music_cv, control = control_resamples(save_pred = TRUE))
```

```{r}
music_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```

```{r,echo = FALSE}
# Calculate accuracy
accuracy <- music_knn %>%
  collect_predictions() %>%
  metrics(truth = playlist, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

# Print the accuracy
print(accuracy)

```
***
I applied a classifier to distinguish the four playlists with different music ablum color. With a k-nearest neighbour classifier,  we can see that the classifier is relatively more effective at identifying music in a white color album, and it has very low performance when identifying blue album. However the overall accuracy is only 20%, which means the probability of accurate prediction is lower than chance. It implies that either there is no clear difference in music with different color albums, or the differences can't be identified from spotify api features.

### Tempograms

```{r}
graveola <- get_tidy_audio_analysis("1lLB0JErnXsekz6uMcAkiH")
```

```{r}
graveola |>
  tempogram(window_size = 8, hop_size = 5, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***
I analyzed the tempogram for one track in the blue corpus. From the graph we can see that the tempo predominantly  centers at 120, but there is one segment where tempo cannot be detected clearly. That is because there is a rhythmic change, and the percussion is relatively weak. 

### Tempo histgram
```{r,echo = FALSE}
library(ggplot2)
library(gridExtra)

# Read playlist data from CSV file
playlist_data <- read.csv("spotify_playlist_features.csv", header = TRUE)
color_palette <- c("pink", "blue","black", "white")
# Function to plot tempo histogram for a playlist
plot_playlist_tempo_histogram <- function(playlist_name, playlist_data, xlim_range) {
  # Extract tempo values for the specified playlist name
  tempo_values <- playlist_data$tempo[playlist_data$playlist_name == playlist_name]
  
  # Create histogram with fixed x-axis range
  ggplot(data.frame(tempo = tempo_values), aes(x = tempo, fill = playlist_name)) +
    geom_histogram(binwidth = 5, fill = color_palette[playlist_name == unique(playlist_data$playlist_name)]) +
    labs(title = playlist_name,
         x = "Tempo (BPM)", y = "Frequency") +
    xlim(xlim_range)
}

# Get maximum and minimum tempo values across all playlists
max_tempo <- max(playlist_data$tempo)
min_tempo <- min(playlist_data$tempo)

# Define the common x-axis range
xlim_range <- c(min_tempo, max_tempo)

# Create empty list to store histograms
histograms <- list()

# Plot tempo histograms for each unique playlist name
unique_playlists <- unique(playlist_data$playlist_name)
for (playlist_name in unique_playlists) {
  histograms[[playlist_name]] <- plot_playlist_tempo_histogram(playlist_name, playlist_data, xlim_range)
}

# Arrange histograms on one page
grid.arrange(grobs = histograms, nrow = 2, ncol = 2)

```

***
The tempo histograms reveal that the tempo across all four corpora tends to center around 120 bpm. However, the distribution of the "blue" corpus appears to be more scattered compared to the others, indicating a greater variability in tempo within that corpus.

### Key histgram


```{r,echo = FALSE}

# Function to convert numeric key to music key
# Define function to convert numeric key to music key
convert_to_music_key <- function(key_numeric) {
  # Define a vector of music keys
  music_keys <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")
  
  # Calculate the index of the music key based on the numeric key
  index <- (key_numeric %% 12) + 1
  
  # Return the corresponding music key
  return(music_keys[index])
}

# Convert numeric key to music key
music$music_key <- sapply(music$key, convert_to_music_key)

# Plot histogram of keys with music keys on x-axis
# Plot histogram of keys with music keys on x-axis
ggplot(music,aes(x = factor(music_key, levels = c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")), fill = factor(mode), group = mode)) +
  geom_bar(data = subset(music, mode == 1 & playlist_name == "new_blue"), aes(y = ..count..), binwidth = 1, color = "black", fill = "blue", stat = "count") +
  geom_bar(data = subset(music, mode == 0 & playlist_name == "new_blue"), aes(y = ..count..), binwidth = 1, color = "black", fill = "lightblue", stat = "count") +
  geom_bar(data = subset(music, mode == 1 & playlist_name == "new_pink"), aes(y = ..count..), binwidth = 1, color = "black", fill = "#FF69B4", stat = "count") +
  geom_bar(data = subset(music, mode == 0 & playlist_name == "new_pink"), aes(y = ..count..), binwidth = 1, color = "black", fill = "#FFB6C1", stat = "count") +
  geom_bar(data = subset(music, mode == 1 & playlist_name == "new_black"), aes(y = ..count..), binwidth = 1, color = "black", fill = "black", stat = "count") +
  geom_bar(data = subset(music, mode == 0 & playlist_name == "new_black"), aes(y = ..count..), binwidth = 1, color = "black", fill = "grey", stat = "count") +
  geom_bar(data = subset(music, mode == 1 & playlist_name == "new_white"), aes(y = ..count..), binwidth = 1, color = "black", fill = "#FFFFD0", stat = "count") +
  geom_bar(data = subset(music, mode == 0 & playlist_name == "new_white"), aes(y = ..count..), binwidth = 1, color = "black", fill = "white", stat = "count") +
  labs(title = "Histogram of Keys by Key Type", x = "Music Key", y = "Frequency", fill = "Key Type") +
  facet_wrap(~ playlist_name, ncol = 2) +  # Separate histograms by playlist_id
  theme_minimal()

```

***
I compared the distribution of keys within the four categories, where the darker color represents the major key and the lighter color represents the minor key. There are more major keys in pink and blue playlists compared to the black and white ones, which might contribute to a higher valence or positive emotional tone. G major is one of the most common keys among the four categories, which differs from most other music genres where C major is the most common. This suggests that there is generally more variety in the sharp and flat pitch classes in my corpus. In the white corpus, there are significantly more C# keys, indicating greater complexity and richness in the music, as the C# key has seven sharps.

### Chordograms

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
library(gridExtra)

sample1 <- get_tidy_audio_analysis("0aNMDYhIWGr0f5yjP6QkUk") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches = map(segments,
                   compmus_summarise, pitches,
                   method = "mean", norm = "manhattan")
  )

plot1 <- sample1 %>%
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if described
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  theme(axis.text.y = element_text(size = 5))

sample2 <- get_tidy_audio_analysis("5HyHcVDIdda1M2SsAPVszp") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches = map(segments,
                  compmus_summarise, pitches,
                  method = "mean", norm = "manhattan")
  )

plot2 <- sample2 %>%
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if described
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  theme(axis.text.y = element_text(size = 5))

# 绘制两个图上下排列
grid.arrange(plot1, plot2, ncol = 1)

```

***
In the chordograms I've compared two songs from the white and pink categories. The first one have a stable pattern where different chords appears at the same time. The second songs shows more variety and circular changes in chord progression. 


### Self Similarity Matrices

```{r, echo = FALSE, message = FALSE, bg = 'transparent', fig.width=10, fig.height=6}
library(tidyverse)
library(spotifyr)
library(compmus)
# Function to create self similarity matrix plot
create_similarity_plot <- function(track_id, title) {
  # Load audio analysis data
  audio_data <- get_tidy_audio_analysis(track_id) %>%
    compmus_align(bars, segments) %>%
    select(bars) %>%
    unnest(bars) %>%
    mutate(
      pitches = map(segments,
                     compmus_summarise, pitches,
                     method = "acentre", norm = "manhattan"),
      timbre = map(segments,
                   compmus_summarise, timbre,
                   method = "mean")
    )

  # Create self similarity matrices for Chroma and Timbre
  chroma_matrix <- audio_data %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma")

  timbre_matrix <- audio_data %>%
    compmus_self_similarity(timbre, "cosine") %>%
    mutate(d = d / max(d), type = "Timbre")

  # Plot self similarity matrices
  plot_chroma <- ggplot(
      chroma_matrix,
      aes(
        x = xstart + xduration / 2,
        width = xduration,
        y = ystart + yduration / 2,
        height = yduration,
        fill = d
      )
    ) +
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(guide = "none") +
    theme_classic() +
    theme(
      panel.background = element_rect(fill = "transparent", color = NA),
      plot.background = element_rect(fill = "transparent", color = NA)
    ) +
    labs(x = "", y = "") +
    ggtitle(paste0("Chroma Self Similarity Matrix - ", title))

  plot_timbre <- ggplot(
      timbre_matrix,
      aes(
        x = xstart + xduration / 2,
        width = xduration,
        y = ystart + yduration / 2,
        height = yduration,
        fill = d
      )
    ) +
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(guide = "none") +
    theme_classic() +
    theme(
      panel.background = element_rect(fill = "transparent", color = NA),
      plot.background = element_rect(fill = "transparent", color = NA)
    ) +
    labs(x = "", y = "") +
    ggtitle(paste0("Timbre Self Similarity Matrix - ", title))

  # Return list of plots
  list(plot_chroma, plot_timbre)
}

# Track IDs and titles
tracks <- list(
  c("6fr1g7TqyLtZC3WqSKovcb", "Blue"),
  c("7kfAbcKnRajKOmuACF3dxl", "Black")
)
theme_set(theme(panel.background = element_rect(fill = "transparent")))

# Create plots for each track
plots <- lapply(tracks, function(track) {
  create_similarity_plot(track[1], track[2])
})

# Display plots in a grid layout
library(gridExtra)


grid.arrange(plots[[1]][[1]], plots[[1]][[2]], plots[[2]][[1]], plots[[2]][[2]], 
             ncol = 2, nrow = 2)

```

***
I've compared two songs from the blue and black categories by analyzing their self-similarity matrices. The first song displays greater overall similarity in melodies and timbre. It shows a clearer structure with distinct segments throughout the track. Notably, the chroma and timbre graphs have similar patterns, indicating that changes in harmony and timbre often occur simultaneously. Conversely, the second song contains smaller segments, making it more challenging to distinguish between them. Additionally, the chroma and timbre matrices depict different patterns, suggesting there are different ways to divide the music from different aspects.It is worth noted that the pattern shows here does not represent the whole category.

### Valence-Energy graph


```{r}
library(gridExtra)
library(ggplot2)
theme_set(theme(panel.background = element_rect(fill = "transparent")))
# 创建四个ggplot对象
pink_plot <- ggplot(pink, aes(x = valence, y = energy)) +
  geom_point() + geom_smooth() +
  coord_cartesian(xlim = c(0,0.8), ylim = c(0, 1)) +
  ggtitle("Pink") +
  theme(plot.title = element_text(hjust = 0.5))
black_plot <- ggplot(black, aes(x = valence, y = energy)) +
  geom_jitter() + geom_smooth() +
  coord_cartesian(xlim = c(0,0.8), ylim = c(0, 1)) +
  ggtitle("Black") +
  theme(plot.title = element_text(hjust = 0.5))

blue_plot <- ggplot(blue, aes(x = valence, y = energy)) +
  geom_jitter() + geom_smooth() +
  coord_cartesian(xlim = c(0,0.8), ylim = c(0, 1)) +
  ggtitle("Blue") +
  theme(plot.title = element_text(hjust = 0.5))

white_plot <- ggplot(white, aes(x = valence, y = energy)) +
  geom_jitter() + geom_smooth() +
  coord_cartesian(xlim = c(0,0.8), ylim = c(0, 1)) +
  ggtitle("White") +
  theme(plot.title = element_text(hjust = 0.5))

# 将ggplot对象转换为grob对象
pink_grob <- ggplotGrob(pink_plot)
black_grob <- ggplotGrob(black_plot)
blue_grob <- ggplotGrob(blue_plot)
white_grob <- ggplotGrob(white_plot)

# 将图形以四宫格的方式排列
grid.arrange(pink_grob, black_grob, blue_grob, white_grob, ncol = 2, nrow = 2,widths = c(0.5, 0.5), heights = c(0.5, 0.5))

```


***
From this graph we can see that the pink category has a combination of sounds with high energy and low valence, or high valence with relatively low energy. The black category has high energy, with valence spread out the scale, but mainly at a low level. The blue category has medium - high energy with medium valence. The white category has medium energy and low valence.


### Introduction page

The corpus I selected is a pool of music tracks i used for DJ sets previously, mainly include deep/ambient techno music. My main interest is to explore the relationship between the design and color themes of the music album covers and the content of the music itself. I chose this topic because I believe that the organic character of this music genre and inspiration from nature would often evoke particular imagery or emotions, which is tightly linked with colors. Additionally a few of the artists are also visual artists, so their album cover designs should strongly align with the music content. I hypothesize that bright and clear color themes on album covers would correspond to brighter music, while darker and more ambiguous cover themes may align with deeper and lower-frequency music. 

I categorized the music tracks in four categories :  black, white, blue and pink,  with 30 tracks in each category. One of the typical examples from the selected soundtracks include the 2022 album "Bloom" and the 2023 album "Groundwork." One album features cover art primarily composed of highly saturated paintings, while the other consists of a purely black abstract geometric design. These two albums also exhibit significant differences in listening experience.


![Alt Text 1](images/black.png){width=275 height=150} ![Alt Text 2](images/blue.png){width=275 height=150} ![Alt Text 3](images/pink.png){width=275 height=150} ![Alt Text 4](images/white.png){width=275 height=150}



